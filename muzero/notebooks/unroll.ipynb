{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (5.2.0)/charset_normalizer (2.0.6) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from muzero.config import make_atari_config\n",
    "from muzero.continous import ContinousActionDecoder, ContinousActionEncoder, ContinousMuzeroNet, VitConfig, tokenizer\n",
    "from muzero.gym_env import create_atari_environment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\utils\\seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\paulh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\utils\\seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted actions: ['action: NOOP', 'action: FIRE', 'action: RIGHT', 'action: LEFT', 'action: RIGHTFIRE', 'action: LEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\accelerate\\utils\\modeling.py:1142: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "C:\\Users\\paulh\\.cache\\huggingface\\modules\\transformers_modules\\NousResearch\\OLMo-Bitnet-1B\\9c9783f0983e51c6dfe84e22c054611ba4eae27f\\model.py:556: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  return F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action embeddings shape:  torch.Size([6, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "runtime_device = \"cuda\"\n",
    "\n",
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "def environment_builder():\n",
    "    return create_atari_environment(\n",
    "        env_name=\"Pong\",\n",
    "        screen_height=224,\n",
    "        screen_width=224,\n",
    "        frame_skip=4,\n",
    "        frame_stack=2,\n",
    "        max_episode_steps=1000,\n",
    "        seed=random_state.randint(1, 2**31),\n",
    "        noop_max=30,\n",
    "        terminal_on_life_loss=False,\n",
    "        clip_reward=False,\n",
    "        output_actions=True,\n",
    "        resize_and_gray=False\n",
    "    )\n",
    "\n",
    "eval_env, eval_actions = environment_builder()\n",
    "\n",
    "config = make_atari_config(\n",
    "        num_training_steps=10,\n",
    "        batch_size=2,\n",
    "        min_replay_size=2,\n",
    "        use_tensorboard=False,\n",
    "        clip_grad=True,\n",
    "    )\n",
    "    \n",
    "formatted_actions = [f\"action: {action}\" for action in eval_actions]\n",
    "print(f\"formatted actions: {formatted_actions}\")\n",
    "\n",
    "tokenized_actions = tokenizer(formatted_actions, padding=True, return_tensors=\"pt\").to(runtime_device)\n",
    "# print(f\"tokenized actions: {tokenized_actions}\")\n",
    "action_encoder = ContinousActionEncoder()\n",
    "action_embeddings = action_encoder(tokenized_actions.input_ids, tokenized_actions.attention_mask)\n",
    "\n",
    "print(\"action embeddings shape: \", action_embeddings.shape)\n",
    "\n",
    "action_decoder = ContinousActionDecoder(action_embeddings)\n",
    "\n",
    "network = ContinousMuzeroNet(\n",
    "    action_encoder,\n",
    "    action_decoder,\n",
    "    action_embeddings.shape[-1],\n",
    "    VitConfig(),\n",
    "    config.num_planes,\n",
    "    config.value_support_size,\n",
    "    config.reward_support_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n"
     ]
    }
   ],
   "source": [
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import multiprocessing\n",
    "from typing import Callable, Iterable, List, Optional\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "from muzero.config import MuZeroConfig\n",
    "from muzero.mcts import uct_search\n",
    "from muzero.network import MuZeroNet\n",
    "from muzero.pipeline import compute_mc_return_target, compute_n_step_target, handle_exit_signal, init_absl_logging, make_unroll_sequence\n",
    "from muzero.replay import Transition\n",
    "from muzero.trackers import make_actor_trackers\n",
    "\n",
    "\n",
    "def make_continous_unroll_sequence(\n",
    "    observations: List[np.ndarray],\n",
    "    actions: List[np.ndarray],\n",
    "    rewards: List[float],\n",
    "    pi_probs: List[np.ndarray],\n",
    "    values: List[float],\n",
    "    priorities: List[float],\n",
    "    unroll_steps: int,\n",
    ") -> Iterable[Transition]:\n",
    "    \"\"\"Turn a lists of episode history into a list of structured transition object,\n",
    "    and stack unroll_steps for actions, rewards, values, MCTS policy.\n",
    "\n",
    "    Args:\n",
    "        observations: a list of history environment observations.\n",
    "        actions: a list of history actual actions taken in the environment.\n",
    "        rewards: a list of history reward received from the environment.\n",
    "        pi_probs: a list of history policy probabilities from the MCTS search result.\n",
    "        values: a list of n-step target value.\n",
    "        priorities: a list of priorities for each transition.\n",
    "        unroll_steps: number of unroll steps during traning.\n",
    "\n",
    "    Returns:\n",
    "        yeilds tuple of structured Transition object and the associated priority for the specific transition.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    T = len(observations)\n",
    "\n",
    "    # States past the end of games are treated as absorbing states.\n",
    "    if len(actions) == T:\n",
    "        actions += [0] * unroll_steps\n",
    "    if len(rewards) == T:\n",
    "        rewards += [0] * unroll_steps\n",
    "    if len(values) == T:\n",
    "        values += [0] * unroll_steps\n",
    "    if len(pi_probs) == T:\n",
    "        absorb_policy = np.ones_like(pi_probs[-1]) / len(pi_probs[-1])\n",
    "        pi_probs += [absorb_policy] * unroll_steps\n",
    "\n",
    "    assert len(actions) == len(rewards) == len(values) == len(pi_probs) == T + unroll_steps\n",
    "\n",
    "    for t in range(T):\n",
    "        end_index = t + unroll_steps\n",
    "        action_sequence = torch.stack([action.cpu().float() for action in actions[t:end_index]])\n",
    "        print(\" ================= action_sequence: \", action_sequence)\n",
    "        print(\" ================= action_sequence shape: \", [action.shape for action in action_sequence])\n",
    "        stacked_action = action_sequence.numpy()\n",
    "        stacked_reward = np.array(rewards[t:end_index], dtype=np.float32)\n",
    "        stacked_value = np.array(values[t:end_index], dtype=np.float32)\n",
    "        stacked_pi_prob = np.array(pi_probs[t:end_index], dtype=np.float32)\n",
    "\n",
    "        yield (\n",
    "            Transition(\n",
    "                state=observations[t],  # no stacking for observation, since it is only used to get initial hidden state.\n",
    "                action=stacked_action,\n",
    "                reward=stacked_reward,\n",
    "                value=stacked_value,\n",
    "                pi_prob=stacked_pi_prob,\n",
    "            ),\n",
    "            priorities[t],\n",
    "        )\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def run_self_play(\n",
    "    config: MuZeroConfig,\n",
    "    rank: int,\n",
    "    network: MuZeroNet,\n",
    "    device: torch.device,\n",
    "    env: gym.Env,\n",
    "    data_queue: multiprocessing.Queue,\n",
    "    train_steps_counter: multiprocessing.Value,\n",
    "    stop_event: multiprocessing.Event,\n",
    "    tag: str = None,\n",
    "    no_mask: bool = False,\n",
    "    action_decoder: Optional[Callable] = None,\n",
    "    action_encoder: Optional[Callable] = None,\n",
    ") -> None:\n",
    "    \"\"\"Run self-play for as long as needed, only stop if `stop_event` is set to True.\n",
    "\n",
    "    Args:\n",
    "        config: a MuZeroConfig instance.\n",
    "        rank: actor process rank.\n",
    "        network: a MuZeroNet instance for acting.\n",
    "        device: PyTorch runtime device.\n",
    "        env: actor's env.\n",
    "        data_queue: a multiprocessing.Queue instance to send samples to leaner.\n",
    "        train_steps_counter: a multiprocessing.Value instance to count current training steps.\n",
    "        stop_event: a multiprocessing.Event instance signals stop run pipeline.\n",
    "        tag: add tag to tensorboard log dir.\n",
    "    \"\"\"\n",
    "\n",
    "    init_absl_logging()\n",
    "    handle_exit_signal()\n",
    "    logging.info(f'Start self-play actor {rank}')\n",
    "\n",
    "    tb_log_dir = f'actor{rank}'\n",
    "    if tag is not None and tag != '':\n",
    "        tb_log_dir = f'{tag}_{tb_log_dir}'\n",
    "\n",
    "    trackers = make_actor_trackers(tb_log_dir) if config.use_tensorboard else []\n",
    "    for tracker in trackers:\n",
    "        tracker.reset()\n",
    "\n",
    "    network = network.to(device=device)\n",
    "    network.eval()\n",
    "    game = 0\n",
    "\n",
    "    while not stop_event.is_set():  # For each new game.\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_trajectory = []\n",
    "        steps = 0\n",
    "\n",
    "        # Play and record transitions.\n",
    "        # the second check is necessary becase the pipeline could have already stopped while the actor is in the middle of a game.\n",
    "        while not done and not stop_event.is_set():\n",
    "            # Make a copy of current player id.\n",
    "            player_id = copy.deepcopy(env.current_player)\n",
    "            # print(\" ================= obs shape: \", obs.shape)\n",
    "            action, pi_prob, root_value = uct_search(\n",
    "                state=obs,\n",
    "                network=network,\n",
    "                device=device,\n",
    "                config=config,\n",
    "                temperature=config.visit_softmax_temperature_fn(steps, train_steps_counter.value),\n",
    "                actions_mask=env.actions_mask,#None if no_mask else env.actions_mask,\n",
    "                current_player=env.current_player,\n",
    "                opponent_player=env.opponent_player,\n",
    "                action_encoder=action_encoder,\n",
    "            )\n",
    "            \n",
    "            # if action_decoder is not None:\n",
    "                # action = action_decoder(action)\n",
    "            # print(\" ================= action: \", action)\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            steps += 1\n",
    "            print (\" ================= iteration: \", steps)\n",
    "            if (steps % 10) == 0:\n",
    "                done = True\n",
    "            action = action if action_encoder is None else action_encoder(action)\n",
    "            for tracker in trackers:\n",
    "                tracker.step(reward, done)\n",
    "\n",
    "            episode_trajectory.append((obs, action, reward, pi_prob, root_value, player_id))\n",
    "            obs = next_obs\n",
    "            \n",
    "            # Send samples to learner every N steps on Atari games.\n",
    "            # Here we accmulate N + unroll_steps + td_steps because\n",
    "            # we needs these extra sequences to compute the target and unroll sequences.\n",
    "            if (\n",
    "                not config.is_board_game\n",
    "                and len(episode_trajectory) == config.acc_seq_length + config.unroll_steps + config.td_steps\n",
    "            ):\n",
    "                # Unpack list of tuples into seperate lists.\n",
    "                observations, actions, rewards, pi_probs, root_values, _ = map(list, zip(*episode_trajectory))\n",
    "                # Compute n_step target value.\n",
    "                target_values = compute_n_step_target(rewards, root_values, config.td_steps, config.discount)\n",
    "\n",
    "                priorities = np.abs(np.array(root_values) - np.array(target_values))\n",
    "\n",
    "                print(\" ================= actions: \", actions[: config.acc_seq_length + config.unroll_steps])\n",
    "                # Make unroll sequences and send to learner.\n",
    "                for transition, priority in make_continous_unroll_sequence(\n",
    "                    observations[: config.acc_seq_length],\n",
    "                    actions[: config.acc_seq_length + config.unroll_steps],\n",
    "                    rewards[: config.acc_seq_length + config.unroll_steps],\n",
    "                    pi_probs[: config.acc_seq_length + config.unroll_steps],\n",
    "                    target_values[: config.acc_seq_length + config.unroll_steps],\n",
    "                    priorities[: config.acc_seq_length + config.unroll_steps],\n",
    "                    config.unroll_steps,\n",
    "                ):\n",
    "                    # data_queue.put((transition, priority))\n",
    "                    print(\" ================= transition: \", transition)\n",
    "                    print(\" ================= priority: \", priority)\n",
    "\n",
    "                del episode_trajectory[: config.acc_seq_length]\n",
    "                del (observations, actions, rewards, pi_probs, root_values, priorities, target_values)\n",
    "\n",
    "        game += 1\n",
    "\n",
    "        # Unpack list of tuples into seperate lists.\n",
    "        observations, actions, rewards, pi_probs, root_values, player_ids = map(list, zip(*episode_trajectory))\n",
    "        \n",
    "        if config.is_board_game:\n",
    "            # Using MC returns as target value.\n",
    "            target_values = compute_mc_return_target(rewards, player_ids)\n",
    "        else:\n",
    "            # Compute n_step target value.\n",
    "            target_values = compute_n_step_target(rewards, root_values, config.td_steps, config.discount)\n",
    "\n",
    "        priorities = np.abs(np.array(root_values) - np.array(target_values))\n",
    "        print(\" =============== full unroll\")\n",
    "        # Make unroll sequences and send to learner.\n",
    "        for transition, priority in make_continous_unroll_sequence(\n",
    "            observations, actions, rewards, pi_probs, target_values, priorities, config.unroll_steps\n",
    "        ):\n",
    "            data_queue.put((transition, priority))\n",
    "\n",
    "        del episode_trajectory[:]\n",
    "        del (observations, actions, rewards, pi_probs, root_values, priorities, player_ids, target_values)\n",
    "\n",
    "    logging.info(f'Stop self-play actor {rank}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0409 00:29:14.819081 31244 1644529528.py:110] Start self-play actor 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ================= iteration:  1\n",
      " ================= iteration:  2\n",
      " ================= iteration:  3\n",
      " ================= iteration:  4\n",
      " ================= iteration:  5\n",
      " ================= iteration:  6\n",
      " ================= iteration:  7\n",
      " ================= iteration:  8\n",
      " ================= iteration:  9\n",
      " ================= iteration:  10\n",
      " =============== full unroll\n",
      " ================= action_sequence:  tensor([[ 0.5000, -0.1348,  1.2188,  ..., -1.5156,  1.1406,  1.3828],\n",
      "        [ 0.6836, -1.2031,  1.2188,  ..., -2.0156,  0.5859,  2.7500],\n",
      "        [ 0.4648, -0.3516,  1.2188,  ..., -0.6562,  1.9766,  1.3828],\n",
      "        [ 0.6836,  0.4941,  1.2188,  ..., -1.2031,  0.9609,  1.3828],\n",
      "        [ 1.0234, -0.1973,  1.2188,  ..., -0.7070,  1.0312,  1.3828]])\n",
      " ================= action_sequence shape:  [torch.Size([2048]), torch.Size([2048]), torch.Size([2048]), torch.Size([2048]), torch.Size([2048])]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from muzero.atari_v2.run_training import ActionEncoderWith\n",
    "from muzero.pipeline import run_training\n",
    "from muzero.replay import PrioritizedReplay\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=config.lr_init, weight_decay=config.weight_decay)\n",
    "lr_scheduler = MultiStepLR(optimizer, milestones=config.lr_milestones, gamma=config.lr_decay_rate)\n",
    "replay = PrioritizedReplay(\n",
    "        10,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        random_state,\n",
    "    )\n",
    "data_queue = multiprocessing.SimpleQueue()\n",
    "train_steps_counter = multiprocessing.Value('i', 0)\n",
    "manager = multiprocessing.Manager()\n",
    "checkpoint_files = manager.list()\n",
    "stop_event = multiprocessing.Event()\n",
    "\n",
    "action_encoder = ActionEncoderWith(action_embeddings)\n",
    "\n",
    "run_self_play(\n",
    "            config,\n",
    "            0,\n",
    "            network,\n",
    "            runtime_device,\n",
    "            eval_env,\n",
    "            data_queue,\n",
    "            train_steps_counter,\n",
    "            stop_event,\n",
    "            \"tag\",\n",
    "            False,\n",
    "            action_decoder,\n",
    "            action_encoder,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
